{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "CycleGAN implementation adapted from Aladdin Persson:\n",
    "\n",
    "- https://www.youtube.com/watch?v=4LktBHGCNfw\n",
    "- https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/GANs/CycleGAN\n",
    "\n",
    "- all comments are mine; changes to the implementation are noted, along with general description of the code\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "This implementation is for a dataset of 256 x 256 patches of higher resolution images of Optical Coherence \n",
    "Tomography (OCT) scans and Gallyas Siver Stain (GSS) sample photographs\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b48f21fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "- utils.py file, only save checkpoint is used\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import random, torch, os, numpy as np\n",
    "import torch.nn as nn\n",
    "import config\n",
    "import copy\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=config.DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6dbd1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "config.py file\n",
    "\n",
    "- Learning rate, lambda, epochs set to authors' original suggestion\n",
    "- transforms updated to use torchvision.transforms rather than albumentations library\n",
    "- file paths updated appropriately\n",
    "- for testing the generated model, the RandomHorizontalFlip line is commented out\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TRAIN_DIR = \"CISL_Dataset/Train_Data\"\n",
    "VAL_DIR = \"CISL_Dataset/Train_Data\"\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 2e-4\n",
    "LAMBDA_CYCLE = 10\n",
    "NUM_WORKERS = 4\n",
    "NUM_EPOCHS = 200\n",
    "LOAD_MODEL = False\n",
    "SAVE_MODEL = True\n",
    "CHECKPOINT_G_GSS = \"g_gss.pth.tar\"\n",
    "CHECKPOINT_F_OCT = \"f_oct.pth.tar\"\n",
    "CHECKPOINT_DISC_GSS = \"disc_gss.pth.tar\"\n",
    "CHECKPOINT_DISC_OCT = \"disc_oct.pth.tar\"\n",
    "\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74b427d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Create a class for the dataset\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class OCT_GSS_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "\n",
    "    - root_OCT, root_GSS: file paths to the directories storing the respective images\n",
    "    -transforms: any augmentations used on the dataset; also includes conversion to tensor\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, root_OCT, root_GSS, transform = None):\n",
    "        self.root_OCT = root_OCT\n",
    "        self.root_GSS = root_GSS\n",
    "        self.transform = transform\n",
    "        \n",
    "        # create lists of files in the image directories\n",
    "        self.OCT_images = os.listdir(root_OCT)\n",
    "        self.GSS_images = os.listdir(root_GSS)\n",
    "        \n",
    "        # delete any other files from the lists of images in the directory\n",
    "        for file in self.OCT_images:\n",
    "            if not file.endswith(\".tif\"):\n",
    "                self.OCT_images.remove(file)\n",
    "                \n",
    "        for file in self.GSS_images:\n",
    "            if not file.endswith(\".tif\"):\n",
    "                self.GSS_images.remove(file)\n",
    "                \n",
    "        self.length_dataset = max(len(self.OCT_images), len(self.GSS_images))\n",
    "        \n",
    "        self.OCT_len = len(self.OCT_images)\n",
    "        self.GSS_len = len(self.GSS_images)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length_dataset\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "\n",
    "        - read each image from the directory\n",
    "        - also return the file name; this is useful to rejoin the patches into the original size images\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # extract one file from the list of the directory, use % len() in case the amount of images in each differs\n",
    "        OCT_img = self.OCT_images[index % self.OCT_len]\n",
    "        GSS_img = self.GSS_images[index % self.GSS_len]\n",
    "        \n",
    "        OCT_path = os.path.join(self.root_OCT, OCT_img)\n",
    "        GSS_path = os.path.join(self.root_GSS, GSS_img)\n",
    "        \n",
    "        # save the name of the file, without the directory\n",
    "        OCT_file = os.path.basename(OCT_path)\n",
    "        GSS_file = os.path.basename(GSS_path)\n",
    "        \n",
    "        OCT_img = np.array(Image.open(OCT_path).convert(\"RGB\"))\n",
    "        GSS_img = np.array(Image.open(GSS_path).convert(\"RGB\"))\n",
    "        \n",
    "        # apply the transforms; this converts the numpy arrays to tensors, so transforms are necessary\n",
    "        if self.transform:\n",
    "            OCT_img = transforms(OCT_img)\n",
    "            GSS_img = transforms(GSS_img)\n",
    "            \n",
    "        return OCT_img, GSS_img, OCT_file, GSS_file\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef6d2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Create classes for the discriminators\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    The discriminator architecture is based on a series of Conv2d-InstanceNorm-LeakyReLU blocks,\n",
    "    which are combined into a block and defined as a class\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # reflection padding is mentioned by the authors as their choice of padding\n",
    "            nn.Conv2d(in_channels, out_channels, 4, stride, 1, bias = True, padding_mode = \"reflect\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    The discriminator class is defined by instantiating a sequence of blocks\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        \n",
    "        # the first layer does not use Instance Normalization, so is defined without using Block\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features[0], kernel_size = 4, stride = 2, padding = 1, padding_mode = \"reflect\"),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "        )\n",
    "        \n",
    "        # the remaining convolution blocks are defined using the specified features and Block objects\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            if feature == features[-1]:\n",
    "                layers.append(Block(in_channels, feature, stride = 1))\n",
    "\n",
    "            else:\n",
    "                layers.append(Block(in_channels, feature, stride = 2))\n",
    "\n",
    "            in_channels = feature\n",
    "        \n",
    "        layers.append(nn.Conv2d(in_channels, 1, kernel_size = 4, stride = 1, padding = 1, padding_mode = \"reflect\"))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        return torch.sigmoid(self.model(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "446fd5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Test the discriminator\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def test():\n",
    "    x = torch.randn((5, 3, 256, 256))\n",
    "    model = Discriminator(in_channels=3)\n",
    "    preds = model(x)\n",
    "    print(preds.shape)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3b6465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Create classes for the generators\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    The convolution blocks combine Conv2d-InstanceNorm_LeakyReLU into blocks, which are used to increase and decrease\n",
    "    the number of features\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, down=True, activation=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, padding_mode = \"reflect\", **kwargs)\n",
    "            \n",
    "            if down\n",
    "            \n",
    "            else nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True) if activation else nn.Identity(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    The residual blocks combine two convolutions, without changing the size\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBlock(channels, channels, kernel_size=3, padding=1),\n",
    "            ConvBlock(channels, channels, activation=False, kernel_size=3, padding=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    The generator is made by instantiating the convolution and residual blocks\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, img_channels, num_features = 64, num_residuals=9):\n",
    "        super().__init__()\n",
    "        # the first layer uses ReLU instead of Leaky ReLU, and is not instantiated from the ConvBlock class\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, num_features, kernel_size = 7, stride = 1, padding = 3, padding_mode = \"reflect\"),\n",
    "            nn.InstanceNorm2d(num_features),\n",
    "            nn.ReLU(inplace = True),\n",
    "        )\n",
    "        \n",
    "        # two ConvBlocks are used to increase the number of features\n",
    "        self.down_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(num_features, num_features*2, kernel_size = 3, stride = 2, padding = 1),\n",
    "                ConvBlock(num_features*2, num_features*4, kernel_size = 3, stride = 2, padding = 1),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # the number of residual blocks varies based on image resolution; for 256 x 256 images, 9 blocks are used\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(num_features*4) for _ in range(num_residuals)]\n",
    "        )\n",
    "        \n",
    "        # two ConvBlocks are used to decrease the number of features\n",
    "        self.up_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(num_features*4, num_features*2, down = False, kernel_size = 3, stride = 2, padding = 1, output_padding = 1),\n",
    "                ConvBlock(num_features*2, num_features, down = False, kernel_size = 3, stride = 2, padding = 1, output_padding = 1),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # a final convolution layer is included to reduce to the number of image channels\n",
    "        self.last = nn.Conv2d(num_features, img_channels, kernel_size = 7, stride = 1, padding = 3, padding_mode = \"reflect\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        for layer in self.down_blocks:\n",
    "            x = layer(x)\n",
    "        x = self.residual_blocks(x)\n",
    "        for layer in self.up_blocks:\n",
    "            x = layer(x)\n",
    "        return torch.tanh(self.last(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "355da157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Test the generator\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def test():\n",
    "    img_channels = 3\n",
    "    img_size = 256\n",
    "    x = torch.randn((2, img_channels, img_size, img_size))\n",
    "    gen = Generator(img_channels, 9)\n",
    "    print(gen(x).shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cfa98c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-025769b39585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-025769b39585>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mD_OCT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0mD_GSS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mG_GSS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_residuals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    985\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    986\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 987\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Implement a train function, and a main function to instantiate all the classes and train the network\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import save_checkpoint, load_checkpoint\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import config\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "The train function trains the network for 1 epoch\n",
    "- D_OCT & D_GSS are the discriminators for the respective image sources\n",
    "- The generator G_GSS converts OCT images to GSS, and F_OCT converts GSS images to OCT\n",
    "- D_optim and G_optim are the optimizers, using Adam\n",
    "- L1 and MSE are the L1 and mean squared error losses\n",
    "- d_scaler and g_scaler scale the step size of the gradient update between epochs\n",
    "\n",
    "\"\"\"\n",
    "def train(D_OCT, D_GSS, G_GSS, F_OCT, loader, D_optim, G_optim, L1, MSE, d_scaler, g_scaler):\n",
    "    \n",
    "    loop = tqdm(loader, leave=True)\n",
    "    \n",
    "    for idx, (OCT, GSS, OCT_file, GSS_file) in enumerate(loop):\n",
    "        \n",
    "        # load one image of each type\n",
    "        OCT = OCT.to(config.DEVICE)\n",
    "        GSS = GSS.to(config.DEVICE)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            \n",
    "            # generate a fake GSS image\n",
    "            GSS_gen = G_GSS(OCT)\n",
    "            \n",
    "            # evaluate the fake and real GSS image with the discriminator\n",
    "            D_GSS_true = D_GSS(GSS)\n",
    "            D_GSS_gen = D_GSS(GSS_gen.detach())\n",
    "            \n",
    "            # calculate the loss for this discriminator\n",
    "            D_GSS_true_loss = MSE(D_GSS_true, torch.ones_like(D_GSS_true))\n",
    "            D_GSS_gen_loss = MSE(D_GSS_gen, torch.zeros_like(D_GSS_gen))\n",
    "            \n",
    "            D_GSS_loss = D_GSS_true_loss + D_GSS_gen_loss\n",
    "            \n",
    "            # generate a fake OCT image and evaluate with the discriminator similarly\n",
    "            OCT_gen = F_OCT(GSS)\n",
    "            \n",
    "            D_OCT_true = D_OCT(OCT)\n",
    "            D_OCT_gen = D_OCT(OCT_gen.detach())\n",
    "            \n",
    "            D_OCT_true_loss = MSE(D_OCT_true, torch.ones_like(D_OCT_true))\n",
    "            D_OCT_gen_loss = MSE(D_OCT_gen, torch.zeros_like(D_OCT_gen))\n",
    "            \n",
    "            D_OCT_loss = D_OCT_true_loss + D_OCT_gen_loss\n",
    "            \n",
    "            # calculate the total loss of both discriminators\n",
    "            D_loss = (D_GSS_loss + D_OCT_loss)/2\n",
    "        \n",
    "        # update the optimizer for the discriminator for each image evaluated \n",
    "        D_optim.zero_grad()\n",
    "        d_scaler.scale(D_loss).backward()\n",
    "        d_scaler.step(D_optim)\n",
    "        d_scaler.update()\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            \n",
    "            # evaluate the loss on the fake generated images\n",
    "            D_GSS_gen = D_GSS(GSS_gen)\n",
    "            D_OCT_gen = D_OCT(OCT_gen)\n",
    "            \n",
    "            loss_G_GSS = MSE(D_GSS_gen, torch.ones_like(D_GSS_gen))\n",
    "            loss_F_OCT = MSE(D_OCT_gen, torch.zeros_like(D_OCT_gen))\n",
    "            \n",
    "            # put the fake images through the reverse generators to compute the cycle consistency loss\n",
    "            cycle_GSS = G_GSS(OCT_gen)\n",
    "            cycle_OCT = F_OCT(GSS_gen)\n",
    "            \n",
    "            # calculate the cycle consistency loss\n",
    "            cycle_loss_GSS = L1(GSS, cycle_GSS)\n",
    "            cycle_loss_OCT = L1(OCT, cycle_OCT)\n",
    "            \n",
    "            # sum the total loss for the generators\n",
    "            G_loss = (loss_G_GSS + loss_F_OCT\n",
    "                     + cycle_loss_GSS * config.LAMBDA_CYCLE\n",
    "                     + cycle_loss_OCT * config.LAMBDA_CYCLE\n",
    "            )\n",
    "        \n",
    "        # update the optimizer for the generator for each image evaluated\n",
    "        G_optim.zero_grad()\n",
    "        g_scaler.scale(G_loss).backward()\n",
    "        g_scaler.step(G_optim)\n",
    "        g_scaler.update()\n",
    "        \n",
    "        # save sample outputs\n",
    "        if idx % 200 == 0:\n",
    "            save_image(GSS_gen * 0.5 + 0.5, f\"saved_images/GSS_{idx}.tif\")\n",
    "            save_image(OCT_gen * 0.5 + 0.5, f\"saved_images/OCT_{idx}.tif\")\n",
    "\n",
    "           \n",
    "# define a learning rate that is constant for 100 epochs, and linearly decays to 0 for the next 100, as described\n",
    "# by the original authors\n",
    "def lr_linear_decay(epoch):\n",
    "    if epoch < 100:\n",
    "        return config.LEARNING_RATE\n",
    "    else:\n",
    "        return config.LEARNING_RATE*(1 - ((epoch - 100) / 100.0))         \n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # define an instance of both discriminators and generators\n",
    "    D_OCT = Discriminator(in_channels = 3).to(config.DEVICE)\n",
    "    D_GSS = Discriminator(in_channels = 3).to(config.DEVICE)\n",
    "    G_GSS = Generator(img_channels = 3, num_residuals = 9).to(config.DEVICE)\n",
    "    F_OCT = Generator(img_channels = 3, num_residuals = 9).to(config.DEVICE)\n",
    "    \n",
    "    # instantiate the optimizers, using betas as described by the original authors\n",
    "    D_optim = optim.Adam(\n",
    "        list(D_OCT.parameters()) + list(D_GSS.parameters()),\n",
    "        lr = config.LEARNING_RATE,\n",
    "        betas = (0.5, 0.999),\n",
    "    )\n",
    "    \n",
    "    G_optim = optim.Adam(\n",
    "        list(G_GSS.parameters()) + list(F_OCT.parameters()),\n",
    "        lr = config.LEARNING_RATE,\n",
    "        betas = (0.5, 0.999),\n",
    "    )\n",
    "    \n",
    "    # define the learning rate schedulers using the above learning rate decay\n",
    "    D_scheduler = lr_scheduler.LambdaLR(D_optim, lr_linear_decay)\n",
    "    \n",
    "    G_scheduler = lr_scheduler.LambdaLR(G_optim, lr_linear_decay)\n",
    "    \n",
    "    # call the L1 and MSE loss functions to use in training\n",
    "    L1 = nn.L1Loss()\n",
    "    MSE = nn.MSELoss()\n",
    "    \n",
    "    # create an option to load a partially trained model (not used)\n",
    "    if config.LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            config.CHECKPOINT_G_GSS,\n",
    "            G_GSS,\n",
    "            G_optim,\n",
    "            config.LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            config.CHECKPOINT_F_OCT,\n",
    "            F_OCT,\n",
    "            F_optim,\n",
    "            config.LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            config.CHECKPOINT_DISC_GSS,\n",
    "            D_GSS,\n",
    "            D_optim,\n",
    "            config.LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            config.CHECKPOINT_DISC_OCT,\n",
    "            D_OCT,\n",
    "            D_optim,\n",
    "            config.LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "    # define an instance of the dataset for training\n",
    "    dataset = OCT_GSS_Dataset(\n",
    "        root_OCT = config.TRAIN_DIR + \"/OCT_Patch\",\n",
    "        root_GSS = config.TRAIN_DIR + \"/GSS_Patch\",\n",
    "        transform = config.transforms,\n",
    "    )\n",
    "    \n",
    "    # define the data loader\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    # define the gradient scalers\n",
    "    g_scaler = torch.cuda.amp.GradScaler()\n",
    "    d_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        \n",
    "        print(\"Epoch: \", epoch)\n",
    "        \n",
    "        train(D_OCT, D_GSS, G_GSS, F_OCT, loader, D_optim, G_optim, L1, MSE, d_scaler, g_scaler)\n",
    "\n",
    "        # update the learning rate as necessary after each epoch\n",
    "        D_scheduler.step()\n",
    "        G_scheduler.step()\n",
    "        \n",
    "        # save and overwrite the model after each epoch\n",
    "        if config.SAVE_MODEL:\n",
    "            save_checkpoint(G_GSS, G_optim, filename=config.CHECKPOINT_G_GSS)\n",
    "            save_checkpoint(F_OCT, G_optim, filename=config.CHECKPOINT_F_OCT)\n",
    "            save_checkpoint(D_GSS, D_optim, filename=config.CHECKPOINT_DISC_GSS)\n",
    "            save_checkpoint(D_OCT, D_optim, filename=config.CHECKPOINT_DISC_OCT)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "562a0d45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:01<00:00, 70.87it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Implement a test function and main function to generate new GSS samples using the trained network\n",
    "It is possible to generate OCT samples as well, but practically there is no use for them, so they are excluded\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def test(G_GSS, loader):\n",
    "    \n",
    "    loop = tqdm(loader, leave=True)\n",
    "    \n",
    "    for idx, (OCT, GSS, OCT_file, GSS_file) in enumerate(loop):\n",
    "        \n",
    "        OCT = OCT.to(config.DEVICE)\n",
    "        GSS = GSS.to(config.DEVICE)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            \n",
    "            GSS_gen = G_GSS(OCT)\n",
    "            \n",
    "            save_image(GSS_gen * 0.5 + 0.5, f\"CISL_Dataset/saved_images/GSSfrom_\" + OCT_file[0])\n",
    "\n",
    "def main():\n",
    "    \n",
    "    test_dataset = OCT_GSS_Dataset(\n",
    "        root_OCT = config.TRAIN_DIR + \"/Test_OCT_Patch\",\n",
    "        root_GSS = config.TRAIN_DIR + \"/Test_GSS_Patch\",\n",
    "        transform = config.transforms,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    GSS_trained = Generator(img_channels = 3, num_residuals = 9).to(config.DEVICE)\n",
    "    checkpoint = torch.load('g_gss.pth.tar')\n",
    "    GSS_trained.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    test(GSS_trained, test_loader)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65b38b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
